{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from path import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"2, 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, latent_dims)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x): #x : torch.Size([64, 2, 400])\n",
    "        x = torch.flatten(x, start_dim=1) # x.shape : torch.Size([64, 800])\n",
    "        x = self.relu(self.fc1(x)) # x.shape : torch.Size([64, 512]) \n",
    "        x = self.fc2(x)\n",
    "        return x #shape = torch.Size([64, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, 800)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.sigmoid(self.fc2(z)) #z.shape : torch.Size([64, 800])\n",
    "        z = torch.reshape(z, (-1, 400, 2)) #뒤에서도 사용할려면 64대신 일반화 해야함.\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRPointData(Dataset):\n",
    "    def __init__(self, root_dir, valid=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.valid = valid\n",
    "        self.files = [_ for _ in os.listdir(root_dir) if _.endswith('.json')]\n",
    "        self.files = self.files[:4000]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __preproc__(self, file):\n",
    "        with open(file, encoding=\"UTF-8\") as f:\n",
    "            \n",
    "            np_points = np.array(json.load(f, strict=False))\n",
    "            nbrs = NearestNeighbors(n_neighbors=11, algorithm='auto').fit(np_points)\n",
    "            matrix = torch.from_numpy(nbrs.kneighbors_graph(np_points).toarray())\n",
    "            points = torch.from_numpy(np_points)\n",
    "            max_val = torch.max(points, -2).values.view(1, -1)\n",
    "            min_val = torch.min(points, -2).values.view(1, -1)\n",
    "            diff = max_val - min_val \n",
    "            points = (points - min_val) / diff\n",
    "        return {'points': points, #[400, 2]\n",
    "               'KNN':matrix}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        json_file = os.path.join(self.root_dir, self.files[idx])\n",
    "        item = self.__preproc__(json_file)\n",
    "        return item\n",
    "    \n",
    "    def __filename__(self, idx):\n",
    "        return self.files[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"data_0610\")\n",
    "train_dr = DRPointData(path)\n",
    "len(train_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=train_dr, batch_size=32, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label.json') as f:\n",
    "    labels = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True,   gridspec_kw={'hspace': 0.1, 'wspace': 0.1})\n",
    "\n",
    "\n",
    "idx = []\n",
    "for i in range(4):\n",
    "    idx.append(random.randint(0, len(train_dr)))\n",
    "\n",
    "df0 = pd.DataFrame(points_data[idx[0]].numpy(), columns=['X', 'Y'])\n",
    "df1 = pd.DataFrame(points_data[idx[1]].numpy(), columns=['X', 'Y'])\n",
    "df2 = pd.DataFrame(points_data[idx[2]].numpy(), columns=['X', 'Y'])\n",
    "df3 = pd.DataFrame(points_data[idx[3]].numpy(), columns=['X', 'Y'])\n",
    "\n",
    "print(points_data[idx[0]].shape) #torch.Size([400, 2])\n",
    "\n",
    "df0.plot.scatter(x='X', y='Y', ax=axes[0,0], s=1, c=labels, cmap='tab10')\n",
    "df1.plot.scatter(x='X', y='Y', ax=axes[0,1], s=1, c=labels, cmap='tab10')\n",
    "df2.plot.scatter(x='X', y='Y', ax=axes[1,0], s=1, c=labels, cmap='tab10')\n",
    "df3.plot.scatter(x='X', y='Y', ax=axes[1,1], s=1, c=labels, cmap='tab10')\n",
    "\n",
    "idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=30):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            #inputs, kNN = data['points'].to(device).float(), data['KNN'].to(device).int()\n",
    "\n",
    "            inputs = data['points'].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = autoencoder(inputs.transpose(1, 2))\n",
    "            loss = ((inputs - x_hat)**2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 5\n",
    "autoencoder = Autoencoder(latent_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = train(autoencoder, dataloader) #TODO AGAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder (nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(800, 512)\n",
    "        self.fc2 = nn.Linear(512, latent_dims)\n",
    "        self.fc3 = nn.Linear(512, latent_dims)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device)\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        mu =  self.fc2(x)\n",
    "        sigma = torch.exp(self.fc3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to train the vae, only need to add the auxillary loss in training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, dataloader, epochs=30):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, kNN = data['points'].to(device).float(), data['KNN'].to(device).int()\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = vae(inputs.transpose(1, 2))\n",
    "            loss = ((inputs - x_hat)**2).sum() + vae.encoder.kl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 5\n",
    "vae = VAE(latent_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = train_vae(vae, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-abc571b6e4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(model, dataloader, latent_dims, num_batches=100):\n",
    "    latent_values = []\n",
    "    latent_numbers = []\n",
    "    for i in range(latent_dims):\n",
    "        latent_values.append(np.array([]))\n",
    "\n",
    "    for i, x in enumerate(dataloader):\n",
    "        x = x['points']\n",
    "        z = model.encoder(x.to(device).float())\n",
    "        z = z.to('cpu').detach().numpy()\n",
    "        # 32 (batch_size)개씩 끊어서\n",
    "        for j in range(latent_dims):\n",
    "            latent_values[j] = np.concatenate((latent_values[j], np.array(z[:, j])))\n",
    "            \n",
    "#        plt.scatter(z[:, 0], z[:, 1], cmap='tab10')\n",
    "        if (i > num_batches):\n",
    "            fig, axes = plt.subplots(nrows=latent_dims, ncols=1, sharex=True, gridspec_kw={'hspace': 0})\n",
    "            for j in range(latent_dims):\n",
    "                print(f'mean: {round(np.mean(latent_values[j]), 4)}, var: {round(np.var(latent_values[j]), 4)}, std :{round(np.std(latent_values[j]), 4)} ')\n",
    "                axes[j].plot(latent_values[j], np.zeros_like(latent_values[j]), 'x')\n",
    "                axes[j].set_ylabel(f'{j}')\n",
    "                #mean, std, 5, 95\n",
    "                latent_numbers.append([np.mean(latent_values[j]), np.std(latent_values[j]), np.percentile(latent_values[j], 5), np.percentile(latent_values[j], 95)])\n",
    "            break\n",
    "            \n",
    "    return latent_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latent_numbers = plot_latent(autoencoder, dataloader, latent_dims)\n",
    "# result_img_ae_latent_distribution.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean: 0.0974, var: 0.0511, std :0.2261 \n",
    "- mean: 0.8144, var: 0.0522, std :0.2284 \n",
    "- mean: 0.3163, var: 0.0634, std :0.2518 \n",
    "- mean: 0.9597, var: 0.0548, std :0.234 \n",
    "- mean: -2.3678, var: 0.2722, std :0.5217 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_latent_numbers = plot_latent(vae, dataloader, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(autoencoder, dims, model, w=24, h=24, n=10, size=50):\n",
    "    fig, axes = plt.subplots(nrows=dims, ncols=n, sharex=True, sharey=True, gridspec_kw={'hspace': 0.1, 'wspace': 0.1})\n",
    "    #fig, axes = plt.subplots(nrows=dims, ncols=n, sharex=True, sharey=True, gridspec_kw={'hspace': 0.1, 'wspace': 0.1})\n",
    "    latent_means = []\n",
    "    fig.set_figheight(h)\n",
    "    fig.set_figwidth(w)\n",
    "#    plt.rcParams[\"figure.figsize\"] = (8,24)\n",
    "#    print(plt.rcParams[\"figure.figsize\"])\n",
    "    if (model == \"ae\"):\n",
    "        latent_numbers = ae_latent_numbers\n",
    "    elif (model == \"vae\"):\n",
    "        latent_numbers = vae_latent_numbers\n",
    "        \n",
    "        \n",
    "    for i in range(dims):\n",
    "        latent_means.append(latent_numbers[i][0])\n",
    "\n",
    "    latent_means = np.array(latent_means)\n",
    "    for i in range(dims):\n",
    "        #latent_mean, latent_std\n",
    "        r0 = (latent_numbers[i][2], latent_numbers[i][3])\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            val = latent_means.copy()\n",
    "            val[i] = x\n",
    "            z = torch.Tensor([val]).to(device)\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            df = pd.DataFrame(x_hat[0].to('cpu').detach().numpy(), columns=['X', 'Y'])\n",
    "            df.plot.scatter(x='X', y='Y', ax=axes[i,j], s=size, c=labels, cmap='tab10')\n",
    "\n",
    "                        \n",
    "    plt.show()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(autoencoder, latent_dims, \"ae\", n=10, h=40, w=50, size=60)\n",
    "# result_img_vae_plot_reconstructed.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(vae,latent_dims, \"vae\", n=10, h=40, w=50, size=60)\n",
    "# result_img_ae_plot_reconstructed.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model/'\n",
    "torch.save(vae.state_dict(), model_path + 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.fc1.weight      :  512\n",
      "encoder.fc1.bias      :  512\n",
      "encoder.fc2.weight      :  5\n",
      "encoder.fc2.bias      :  5\n",
      "encoder.fc3.weight      :  5\n",
      "encoder.fc3.bias      :  5\n",
      "decoder.fc1.weight      :  512\n",
      "decoder.fc1.bias      :  512\n",
      "decoder.fc2.weight      :  800\n",
      "decoder.fc2.bias      :  800\n"
     ]
    }
   ],
   "source": [
    "for key, value in vae.state_dict().items():\n",
    "    print(key,\"     : \", value.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute latent value for all input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = train_dr.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file, encoding=\"UTF-8\") as f:\n",
    "        np_points = np.array(json.load(f, strict=False))\n",
    "        points = torch.from_numpy(np_points)\n",
    "        max_val = torch.max(points, -2).values.view(1, -1)\n",
    "        min_val = torch.min(points, -2).values.view(1, -1)\n",
    "        diff = max_val - min_val \n",
    "        points = (points - min_val) / diff #[400, 2]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_vector(file_name, n_trial=10):\n",
    "#    print(file_name)\n",
    "    root_dir = Path(\"data_0610\")\n",
    "    file_dir = os.path.join(root_dir, file_name)\n",
    "    \n",
    "    with open(file_dir, encoding=\"UTF-8\") as f:\n",
    "        np_points = np.array(json.load(f, strict=False))\n",
    "        points = torch.from_numpy(np_points)\n",
    "        max_val = torch.max(points, -2).values.view(1, -1)\n",
    "        min_val = torch.min(points, -2).values.view(1, -1)\n",
    "        diff = max_val - min_val \n",
    "        points = (points - min_val) / diff #[400, 2]\n",
    "        points = points.to('cpu').float()\n",
    "        x = torch.Tensor(points.transpose(0, 1))\n",
    "        x.resize_(1, x.shape[0] * x.shape[1])\n",
    "        \n",
    "        arr = np.empty((1,5), float)\n",
    "        for i in range(n_trial):\n",
    "            z = vae.encoder(torch.Tensor(x).to(device).float()).to('cpu').detach().numpy()\n",
    "            arr = np.append(arr, z, axis=0)\n",
    "\n",
    "        arr = np.delete(arr, [0, 0], axis=0)\n",
    "        \n",
    "        \n",
    "        return torch.Tensor(arr.mean(axis=0)).resize_(1, 5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector = np.empty((0,5), float)\n",
    "method_list = np.empty(0, float)\n",
    "\"\"\"\n",
    "0~999: umap\n",
    "1000-1999: tsne\n",
    "2000-2999: isomap\n",
    "3000-3999: densmap\n",
    "4000-4999: lle\n",
    "\"\"\"\n",
    "\n",
    "for file in files:\n",
    "    z = append_vector(file)\n",
    "    latent_vector = np.append(latent_vector, z, axis=0)\n",
    "    num = int(file[6:].split('.')[0])\n",
    "    method_num = int(num / 1000)\n",
    "    method_list = np.append(method_list, method_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('latent_vector.json', 'w') as f:\n",
    "    json.dump(latent_vector.tolist(), f)\n",
    "    \n",
    "with open ('method_num.json', 'w') as f:\n",
    "    json.dump(method_list.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
